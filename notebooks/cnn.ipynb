{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540ceaad-989e-432f-92f5-c13c85af46cb",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network** \\\n",
    "Recognize images of numbers from mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b75a49-acda-4115-83c8-d6a4f299361b",
   "metadata": {},
   "source": [
    "**Data Wrangling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf5f688-9af7-4d01-82ea-a5d44bb284c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88e48aa-2928-48bf-9486-8a10c8b2e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe6df27-3699-4eef-81c3-8762b67043b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6872d691-71f7-437e-8446-d919d576a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.func import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af931cb4-0fa4-4155-9ce8-e10293e6407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can use the GPU.\n",
      "Number of GPUs: 1\n",
      "Current GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA Version built with PyTorch: 12.6\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use the GPU.\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version built with PyTorch: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d22169c7-abc7-481f-a756-e854205aecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set()\n",
    "for l in ds['train']['label']:\n",
    "    labels.add(l)\n",
    "output_range = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08936a5d-bff0-4b48-b0a7-8757eb41455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pngs to tensors containing pixel values\n",
    "rows = len(ds['train'])\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "images = torch.empty((rows, width, height), dtype=torch.int64)\n",
    "for i, row in enumerate(ds['train'].select(range(rows))):\n",
    "    images[i] = torch.reshape(torch.tensor(list(row['image'].getdata())), (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac20428-f4e1-4303-a80f-349319442f54",
   "metadata": {},
   "source": [
    "**Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f88f291-9e23-4831-bb7c-08c3d984662b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to divide out rows from first dimension of tensor\n",
    "def seperate(tensor: torch.Tensor, i: int):\n",
    "    shape = tensor.shape\n",
    "    if len(shape) > 1:\n",
    "        return tensor.reshape(i, shape[0]//i, *shape[1:])\n",
    "    return tensor.reshape(i, shape[0]//i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4381078-7668-4c80-9a14-789ede43cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernels\n",
    "torch.manual_seed(3500)\n",
    "min_fence = -0.2\n",
    "max_fence = 0.2\n",
    "\n",
    "# initialize kernels with random values and transform them to range [-0.2, 0.2)\n",
    "kernel_l1 = torch.rand(2, 1, 5, 5) * (max_fence - min_fence) + min_fence\n",
    "kernel_l2 = torch.rand(4, 1, 3, 3) * (max_fence - min_fence) + min_fence\n",
    "\n",
    "# initialize biases\n",
    "bias_l1 = torch.rand(2, 1) * (max_fence - min_fence) + min_fence\n",
    "bias_l2 = torch.rand(4, 1) * (max_fence - min_fence) + min_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1391225-cf8f-46f5-a785-6199d24567bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 5, 5])\n",
      "torch.Size([4, 1, 3, 3])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{kernel_l1.shape}\\n{kernel_l2.shape}\\n{bias_l1.shape}\\n{bias_l2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9f95ac-436d-4bfd-91a3-27e467b76dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes convolution between kernel and matrix\n",
    "def traverse_matrix(matrix: torch.Tensor, kernel: torch.Tensor, step: int) -> torch.Tensor:\n",
    "    if len(kernel.shape) == 2:\n",
    "        print('warning: kernel rank is 2, adjusting shape')\n",
    "        kernel = kernel.unsqueeze(0)\n",
    "    if len(kernel.shape) != 3:\n",
    "        raise Exception(f\"kernel has a rank of {len(kernel.shape)}\")\n",
    "    \n",
    "    width, height = matrix.shape[1] - kernel.shape[-2] + 1, matrix.shape[0] - kernel.shape[-1] + 1\n",
    "\n",
    "    # create tensor of all windows kernel will slide over and repeat by number of kernels\n",
    "    matrix_unfolded = (\n",
    "        matrix\n",
    "            .unfold(0, width, step)\n",
    "            .unfold(1, height, step)\n",
    "            .permute(2,3,0,1)\n",
    "    )\n",
    "\n",
    "    matrix_unfolded_repeated = (\n",
    "        matrix_unfolded\n",
    "            .repeat(kernel.shape[0],1,1,1)\n",
    "            .reshape(kernel.shape[0], *matrix_unfolded.shape)\n",
    "    )\n",
    "\n",
    "    kernel = kernel.unsqueeze(1).unsqueeze(1)\n",
    "    unfolded_sum = kernel * matrix_unfolded\n",
    "    return unfolded_sum.sum(dim=(-1,-2)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca7c821-2bd0-4424-8d05-24cba3288633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(matrix: torch.Tensor, pool_size: tuple) -> torch.Tensor:\n",
    "    if len(matrix.shape) != 2:\n",
    "        raise Exception(f\"Matrix has a rank of {len(matrix.shape)}\")\n",
    "    if matrix.shape[0] % pool_size[0] != 0 or matrix.shape[1] % pool_size[1] != 0:\n",
    "        raise Exception(f\"Pool size {pool_size} is not a multiple of matrix shape {matrix.shape}\")\n",
    "\n",
    "    pool_h, pool_w = pool_size\n",
    "    pooled_h, pooled_w = matrix.shape[0] // pool_h, matrix.shape[1] // pool_w\n",
    "    \n",
    "    return (\n",
    "        matrix\n",
    "        .reshape(pooled_h, pool_h, pooled_w, pool_w)\n",
    "        .max(dim=3)\n",
    "        .values\n",
    "        .max(dim=1)\n",
    "        .values\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b93bb8-ba89-4075-afbb-607910cc5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "def forward(\n",
    "    data: torch.Tensor, \n",
    "    kernel_stacks: list, \n",
    "    weights: torch.Tensor, \n",
    "    biases: list, \n",
    "    activation_functions: list, \n",
    "    pool_size: tuple\n",
    ") -> list:\n",
    "    CHANNEL_INDEX = 1\n",
    "    MS_PER_MATRIX = 568.25\n",
    "    print(f\"ETA: {round(len(data) * (MS_PER_MATRIX / 1000 / 60), 2)} minutes\")\n",
    "    \n",
    "    if not (isinstance(data, torch.Tensor) or isinstance(data, list)):\n",
    "        raise Exception(f\"Data is not of type 'list', inputted type is: {type(data)}\")\n",
    "    if not isinstance(kernel_stacks, list):\n",
    "        raise Exception(f\"Kernels is not of type 'list', inputted type is: {type(kernel_stacks)}\")\n",
    "    if len(kernel_stacks) > len(biases):\n",
    "        raise Exception(f'number of kernel stacks {len(kernel_stacks)} greater than number of biases ({len(biases)})')\n",
    "    if len(kernel_stacks) != len(activation_functions):\n",
    "        print(f\"Number of kernel stacks inputted ({len(kernel_stacks)}) does \" +\n",
    "            f\"not equal number of activation functions inputted \" +\n",
    "            f\"({len(activation_functions)})\")\n",
    "\n",
    "    images_processed = len(data)\n",
    "    for i, kernel_stack in enumerate(kernel_stacks):\n",
    "        kernel_stacks[i] = kernel_stack.unsqueeze(0).expand(images_processed, *kernel_stack.shape)\n",
    "        kernel_stacks[i] = kernel_stacks[i].reshape(-1, *kernel_stacks[i].shape[-3:])\n",
    "\n",
    "    for i, bias in enumerate(biases):\n",
    "        if i < len(kernel_stacks):\n",
    "            biases[i] = (\n",
    "                bias\n",
    "                .repeat(images_processed,1)\n",
    "                .unsqueeze(-1)\n",
    "            )\n",
    "    \n",
    "    # define working input\n",
    "    input_stack = data\n",
    "    output = [None for _ in range(len(kernel_stacks)+1)]\n",
    "    for i, kernel_stack in enumerate(kernel_stacks):\n",
    "        # initialize activation func\n",
    "        func = torch.nn.ReLU()\n",
    "        if i < len(activation_functions):\n",
    "            if activation_functions[i].lower() == 'sigmoid':\n",
    "                func = torch.nn.Sigmoid()\n",
    "            elif activation_functions[i].lower() != 'relu':\n",
    "                raise Exception(f'Activation function \"{activation_functions[i]}\" ' +\n",
    "                               f\"is not a valid activation function.\")\n",
    "        print(f'scale: {kernel_stack.shape[0] // input_stack.shape[0]}')\n",
    "        input_stack = input_stack.repeat_interleave(kernel_stack.shape[0] // input_stack.shape[0], dim=0)\n",
    "        print(f'input: {input_stack.shape}\\nkernel: {kernel_stack.shape}')\n",
    "        \n",
    "        # take convolution\n",
    "        convolution = vmap(traverse_matrix, in_dims=(0,0,None))(input_stack, kernel_stack, 1) \n",
    "        print(convolution.shape)\n",
    "        convolution = convolution + biases[i]\n",
    "        # take activation\n",
    "        activation = func(convolution)\n",
    "        # pool activation\n",
    "        pooled = vmap(max_pool, in_dims=(0,None))(activation, (2,2))\n",
    "\n",
    "        # update output\n",
    "        output[i] = (convolution, activation, pooled)\n",
    "        # update working input\n",
    "        input_stack = pooled\n",
    "        print(f'\\nconvolution: {convolution.shape}\\nactivation: {activation.shape}\\npooled: {pooled.shape}\\n')\n",
    "\n",
    "    # flattening\n",
    "    flattened = (\n",
    "        seperate(pooled, images_processed)\n",
    "        .flatten(1)\n",
    "        .unsqueeze(-1)\n",
    "    )\n",
    "\n",
    "    # initialize weights and biases for fully connected layer if not already\n",
    "    if weights is None and len(biases) == len(kernel_stacks):\n",
    "        torch.manual_seed(3500)\n",
    "        min_fence = -0.2\n",
    "        max_fence = 0.2\n",
    "        \n",
    "        weights = torch.rand(output_range, flattened.shape[1]) * (max_fence - min_fence) + min_fence\n",
    "        biases.append(torch.rand(output_range, 1) * (max_fence - min_fence) + min_fence)\n",
    "\n",
    "    # compute fully connected layer output\n",
    "    eps = 1e-7\n",
    "    fc_output = (weights @ flattened.T + biases[-1]).squeeze(0)\n",
    "    func = torch.nn.Sigmoid()\n",
    "    activation = torch.clamp(\n",
    "        func(fc_output), \n",
    "        eps, \n",
    "        1-eps\n",
    "    )\n",
    "    output[-1] = (flattened, weights, biases, activation)\n",
    "    \n",
    "    print(f'flattened: {flattened.shape}\\nweights: {weights.shape}\\nbiases: {biases[-1].shape}\\nactivation: {activation.shape}]\\n')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425cd1e-1175-4052-9093-240e6feea004",
   "metadata": {},
   "source": [
    "**Back Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10fd695f-ffa4-4631-aa1b-3674866a3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross-entropy loss function\n",
    "def compute_loss(a: torch.Tensor, y: torch.Tensor):\n",
    "    return y * torch.log(a) + (1 - y) * torch.log(1 - a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6629913e-732d-4e09-8509-9e10a65ffa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute cost of output for all images\n",
    "def compute_cost(output: torch.Tensor, dataset: DatasetDict):\n",
    "    cost = torch.zeros(output_range)\n",
    "    images_computed = output.shape[-1]\n",
    "    for i in range(output.shape[1]):\n",
    "        label = dataset['train'][i]['label']\n",
    "        eps = 1e-7\n",
    "        output_v = torch.clamp(output[:, i], eps, 1-eps)\n",
    "        expected_v = torch.zeros(output_range)\n",
    "        expected_v[label] = 1\n",
    "        \n",
    "        cost += compute_loss(output_v, expected_v)\n",
    "    cost /= images_computed\n",
    "    return -cost.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7134415f-5f1e-4ab8-9a48-dfa961ef357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute delta for pooled matrix (dA/dL)\n",
    "def max_pool_d(matrix: torch.Tensor, pooled_d: torch.Tensor, pool_size: tuple) -> torch.Tensor:\n",
    "    if len(matrix.shape) != 2:\n",
    "        raise Exception(f\"Matrix has a rank of {len(matrix.shape)}\")\n",
    "    if len(pool_size) != 2:\n",
    "        raise Exception(f\"Pool size {pool_size} has a rank of {len(pool_size)}\")\n",
    "        \n",
    "    pool_h, pool_w = pool_size\n",
    "    if matrix.shape[0] % pool_h != 0 or matrix.shape[1] % pool_h != 0:\n",
    "        raise Exception(f\"Pool size {pool_size} is not a multiple of matrix shape {matrix.shape}\")\n",
    "\n",
    "    matrix_d = torch.zeros_like(matrix)\n",
    "    for i in range(0, matrix.shape[0] - pool_h + 1, pool_h):\n",
    "        for j in range(0, matrix.shape[1] - pool_w + 1, pool_w):\n",
    "            window = matrix[i:i+pool_h, j:j+pool_w]\n",
    "            _, idx = window.reshape(-1).max(dim=0)\n",
    "            r, c = idx // window.size(1) + i, idx % window.size(1) + j\n",
    "\n",
    "            matrix_d.index_put_(\n",
    "                (r, c), \n",
    "                pooled_d[i // pool_h, j // pool_w],\n",
    "                accumulate=True\n",
    "            )  \n",
    "\n",
    "    return matrix_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cf94e27-ad20-4730-958c-9cadfd3d17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute delta for post ReLU matrix\n",
    "def relu_d(matrix: torch.Tensor):\n",
    "    matrix_d = torch.zeros_like(matrix)\n",
    "    matrix_d[matrix > 0] = 1\n",
    "    return matrix_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5803494d-155b-455c-b2b4-f140dc016a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(forward_output: list, kernel_stacks: list, input_x: torch.Tensor, dataset: DatasetDict, output_range: int):\n",
    "    # compute loss\n",
    "    activation_fc = forward_output[-1][3]\n",
    "    cost = compute_cost(activation_fc, dataset)\n",
    "    images_computed = activation_fc.shape[-1]\n",
    "    \n",
    "    # compute fully connected layer deltas\n",
    "    biases_fc_d = None\n",
    "    flattened, weights, biases_fc = forward_output[-1][0], forward_output[-1][1], forward_output[-1][2]\n",
    "\n",
    "    labels = torch.tensor(ds['train']['label'], dtype=torch.long)\n",
    "    expected = torch.zeros(output_range, images_computed)\n",
    "    expected[labels[:images_computed], torch.arange(images_computed)] = 1\n",
    "    \n",
    "    output_fc_d = activation_fc - expected #dZ, kept seperate per image\n",
    "    weights_d = output_fc_d @ flattened.squeeze() / images_computed #dW, averaged\n",
    "    biases_fc_d = torch.sum(output_fc_d, dim=1) / images_computed #dB, averaged\n",
    "    flattened_d = weights.T @ output_fc_d #dF, kept seperate\n",
    "    \n",
    "    print(f'fully connected layer\\noutput deltas: {output_fc_d.shape}\\nweights deltas: {weights_d.shape}')\n",
    "    print(f'biases: {biases_fc_d.shape}\\nflattened deltas: {flattened_d.shape}\\n')\n",
    "    \n",
    "    # compute convolutional pass deltas, iterate backwards starting from last convolutional layer\n",
    "    deltas = [None for _ in range(len(kernel_stacks))]\n",
    "    for i, cl_output in reversed(list(enumerate(forward_output[:-1]))):\n",
    "        print(f'layer {i}')\n",
    "        convolution, activation, pooled = forward_output[i]\n",
    "        \n",
    "        # compute delta for pooled matrix\n",
    "        if i == len(kernel_stacks)-1:\n",
    "            # if at last convolutional layer, compute delta based on flattened\n",
    "            pooled_d = flattened_d.T.reshape(pooled.shape)\n",
    "        else:\n",
    "            # if at a middle convolutional layer, use past delta\n",
    "            kernel, prev_kernel = kernel_stacks[i], kernel_stacks[i+1]\n",
    "            prev_convolution_d = deltas[i+1][0]\n",
    "            \n",
    "            padding = (pooled.shape[-1] + prev_kernel.shape[-1] - prev_convolution_d.shape[-1] - 1) // 2\n",
    "            prev_convolution_d_padded = F.pad(prev_convolution_d,(padding,)*4,value=0)\n",
    "            rotated_kernel = (\n",
    "                prev_kernel\n",
    "                .flatten(0,1)\n",
    "                .rot90(2,(-2,-1))\n",
    "            )\n",
    "\n",
    "            pooled_d_temp = (\n",
    "                seperate(\n",
    "                    vmap(traverse_matrix, in_dims=(0,0,None))(prev_convolution_d_padded, rotated_kernel, 1),\n",
    "                    images_processed\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            channel_f = prev_kernel.shape[0] // kernel.shape[0]\n",
    "            s = pooled_d_temp.shape\n",
    "            pooled_d = (\n",
    "                pooled_d_temp\n",
    "                    .reshape(s[0], channel_f, s[1] // channel_f, *s[2:])\n",
    "                    .sum(2)\n",
    "                    .flatten(0,1)\n",
    "            )\n",
    "\n",
    "        # compute deltas for layer 2 activations\n",
    "        activation_d = seperate(\n",
    "            vmap(max_pool_d, in_dims=(0, 0, None))(activation, pooled_d, (2,2)),\n",
    "            images_processed\n",
    "        )\n",
    "\n",
    "        # compute activation deltas for convolutionns\n",
    "        convolution_d_activation = seperate(relu_d(convolution), images_computed)\n",
    "\n",
    "        # compute cost deltas for convolutions\n",
    "        convolution_d = (convolution_d_activation * activation_d).flatten(0,1)\n",
    "\n",
    "        if i == 0:\n",
    "            # if at the beginning convolutional layer, input tensor is the input to the convolutional neuron\n",
    "            input_t = input_x.repeat_interleave(2, dim=0) \n",
    "        else:\n",
    "            # next pooled matrix is input to convolutional neuron\n",
    "            input_t = forward_output[i-1][2].repeat_interleave(2, dim=0) \n",
    "\n",
    "        kernel_d = seperate(\n",
    "            vmap(traverse_matrix, in_dims=(0,0,None))(input_t, convolution_d, 1),\n",
    "            images_processed\n",
    "        )\n",
    "\n",
    "        # compute deltas for layer 2 biases, averaged and used to update biases\n",
    "        convolution_d_seperated = seperate(convolution_d, images_processed)\n",
    "        bias_d = convolution_d_seperated.sum((0,2,3)) / images_processed\n",
    "        \n",
    "        print(f'pooled deltas: {pooled_d.shape}\\nactivation deltas: {activation_d.shape}\\nconvolution deltas: {convolution_d.shape}')\n",
    "        print(f'kernel deltas: {kernel_d.shape}\\nbias deltas: {bias_d.shape}\\n')\n",
    "        deltas[i] = (convolution_d, kernel_d, bias_d)\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20099b6b-dc17-471f-9e83-950bd52f911d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 0.03 minutes\n",
      "scale: 2\n",
      "input: torch.Size([6, 28, 28])\n",
      "kernel: torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6, 24, 24])\n",
      "\n",
      "convolution: torch.Size([6, 24, 24])\n",
      "activation: torch.Size([6, 24, 24])\n",
      "pooled: torch.Size([6, 12, 12])\n",
      "\n",
      "scale: 2\n",
      "input: torch.Size([12, 12, 12])\n",
      "kernel: torch.Size([12, 1, 3, 3])\n",
      "torch.Size([12, 10, 10])\n",
      "\n",
      "convolution: torch.Size([12, 10, 10])\n",
      "activation: torch.Size([12, 10, 10])\n",
      "pooled: torch.Size([12, 5, 5])\n",
      "\n",
      "flattened: torch.Size([3, 100, 1])\n",
      "weights: torch.Size([10, 100])\n",
      "biases: torch.Size([10, 1])\n",
      "activation: torch.Size([10, 3])]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atman\\AppData\\Local\\Temp\\ipykernel_28264\\2534363671.py:87: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4419.)\n",
      "  fc_output = (weights @ flattened.T + biases[-1]).squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fully connected layer\n",
      "output deltas: torch.Size([10, 3])\n",
      "weights deltas: torch.Size([10, 100])\n",
      "biases: torch.Size([10])\n",
      "flattened deltas: torch.Size([100, 3])\n",
      "\n",
      "layer 1\n",
      "warning: kernel rank is 2, adjusting shape\n",
      "pooled deltas: torch.Size([12, 5, 5])\n",
      "activation deltas: torch.Size([3, 4, 10, 10])\n",
      "convolution deltas: torch.Size([12, 10, 10])\n",
      "kernel deltas: torch.Size([3, 4, 3, 3])\n",
      "bias deltas: torch.Size([4])\n",
      "\n",
      "layer 0\n",
      "warning: kernel rank is 2, adjusting shape\n",
      "warning: kernel rank is 2, adjusting shape\n",
      "pooled deltas: torch.Size([6, 12, 12])\n",
      "activation deltas: torch.Size([3, 2, 24, 24])\n",
      "convolution deltas: torch.Size([6, 24, 24])\n",
      "kernel deltas: torch.Size([3, 2, 5, 5])\n",
      "bias deltas: torch.Size([2])\n",
      "\n",
      "CPU times: total: 2.19 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "kernel_stacks = [kernel_l1, kernel_l2]\n",
    "biases = [bias_l1, bias_l2]\n",
    "images_processed, kernel_layers = 3, len(kernel_stacks)\n",
    "\n",
    "activation_functions = ['relu', 'relu']\n",
    "input_x = images[:images_processed]\n",
    "output_conv = forward(input_x, kernel_stacks, None, biases, activation_functions, (2,2))\n",
    "deltas = backward(output_conv, kernel_stacks, input_x, ds, output_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ae589-5963-4ac2-b7d1-f04f21aa800a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
